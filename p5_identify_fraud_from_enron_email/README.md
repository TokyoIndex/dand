
# Investigating the Enron Corpus Using Machine Learning for Fun and Profit

Nicholas Cica - nicholasjcica [at] gmail.com

## Introduction

The Enron Corpus is fascinating because it is a time capsule of email data – back before everyone had converted to a digital lifestyle, people were still commutating honestly without the idea that their online presence or reputation could be affected. With over 600,000 emails generated by 158 employees, the Enron Corpus provide a glimpse into the inner working of one of the largest companies in America. It is important to Machine Learning Researchers because it provides a treasure trove of real world data that can be used to make predictions about human behavior.

## Explore the Enron Dataset

The goal of the Enron Corpus project is to predict which Enron Employees are “Persons Of Interest” (POI) in the Enron fraud case.  We will use Supervised Machine Learning to learn from a training set of labeled data, and then predict which employees are POI by evaluating a learning algorithm on a training set of data. 

Initial exploration of the Enron data corpus yield 14 financial features and 6 email features. We will use Machine Learning to generate “weights” for each of these features to evaluate their importance in making predictions about the data.  This in turn will then help predict which employees are POIs.  The process will prove challenging because there is an uneven distribution of POIs in the data.  There are only 18 POI and 128 Non-POI out of a total of 146 employee data points. 

## Data Processing

The first step is to explore the dataset. As I mentioned, the features consist of financial features such as ‘salary’ and ‘bonus’ and email features such as ‘to emails’ and ‘from emails’.  Running a few inquiries on the data, I was able to identify few outliers that should be removed from the data.  The **TOTAL** row represents the total values for all the employees, so it can clearly be removed since the total cannot be a POI.  Additionally, **The Travel Agency in the Park** is not an employee, so its values were also removed.  Finally, **Eugene E Lockheart** as an interesting case because they did not have a value for any of the financial features. In the end, I decided to remove this person from dataset because I felt they could incorrectly skew the data.  Finally, missing Financial data will be converted to zero values by the feature format function.

### Selecting Features

Since the features are one of our hyper parameters, selecting the correct features is extremely important. To figure out which features are most significant to our learning algorithm, I used the [**SelectKBest**](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) algorithm to identify and rank the most influential features within the dataset. 

Ranking| Feature | Score 
---| --- | --- 
1|Shared Receipt With POI | 8.9038
2|From POI to this Person | 5.4466
3|Loan Advances | 2.5182
4|From this Person to POI | 2.4705
5|To Messages | 1.7516
6|Director Fees | 0.5490
7|Total Payments | 0.3496
8|Deferral Payments | 0.2389
9|Exercised Stock  Options | 0.2282
10|Deferred Income | 0.2195
11|Total Stock Value | 0.1661
12|From Messages | 0.1587
13|Bonus | 0.0779
14|Other | 0.0681
15|Restricted_Stock | 0.0313
16|Long Term Incentive | 0.0222
17|Expenses | 0.0139
18|Restricted Stock Deferred | 0.0041
19|Salary | 0.0001

When visualizing the SelectKBest Feature scores, there is a clear drop in performance after the top five scores:

<img src="feature_scatter.jpg">

As I added additional features to be evaluated by the algorithm, I recorded the **Precision** and **Recall**.  For this exercise, I used the **Gaussian Naiive Bayes** algorithm to generate the initial results.  This will help us find which features contribute the most to precision and recall.

Ranking| Feature | Precision | Recall 
---| --- | --- 
1|Shared Receipt With POI | error | error
2|From POI to this Person | 0.00339 | 0.00200
3|Loan Advances | error | error
4|From this Person to POI | error | error
5|To Messages | error | error
6|Director Fees | 0.19038 | 0.76800
7|Total Payments | 0.14755 | 0.82400
8|Deferral Payments | 0.15423 | 0.81400
9|Exercised Stock Options |  0.14552 | 0.82300
10|Deferred Income | 0.14509 | 0.82050
11|Total Stock Value | 0.14195 | 0.81850
12|From Messages |  0.14276 | 0.82050
13|Bonus | 0.14134 |  0.81850
14|Other | 0.14139 | 0.81700
15|Restricted_Stock | 0.14146 | 0.81900
16|Long Term Incentive | 0.14143 | 0.81900
17|Expenses | 0.14186 | 0.82250
18|Restricted Stock Deferred | 0.15593 | 0.83600
19|Salary | 0.15599 | 0.83400

Next I visualized the performance of the features:

<img src="precision_recall.jpg">

Using the top six features seems to be the best compromise of precision and recall. I also noticed that four of the five features were email features, so I decided to focus my attention on these features over the financial features.

### Creating New Features

Another important step to consider is whether to generate new features by combining existing features.  These "new features" are able to combine their explanatory power by scaling two points into one. I chose to create to new features, **Fraction to POI** and **Fraction from POI**, by calculating the ratio of message to a POI and from a POI over all the message sent or received by the employee.  These new features will provide a better representation of the importance of exchanging email with a POI by scaling the email messages to be equal across all the employees and account for employees that send and receive with a low or high volume of emails.

Including these two new features did not seem to impact accuracy, precision and recall in the learning algorithms. I decided to remove the new features.

New Feature | Accuracy | Precision | Recall| F1 Score
--- | --- | --- | --- | ---
Without  | 0.36400 | 0.19038 | 0.76800 | 0.30513
With  | 0.36400 |  0.19038 | 0.76800 | 0.30513

## Selecting and Tuning Algorithms

Next, we need to test the different algorithms, or differently tuned versions of algorithm to maximize the results. I chose to focus on comparing four different algorithms to see which one was better at classifying the employees as POI or Non-POI:

>**Naive Bayes** methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features.

>The principle behind **K Nearest Neighbor** methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. 

>The goal of the **Decision Tree Classifier** is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.

>**Linear Regression** fits a linear model with coefficients to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.

After the initial tests, **K Nearest Neighbor** provided the highest overall accuracy, but when looking at other factors such as precision, recall and F1 score, the **Decision Tree Classifier** algorithm and the **Logistic Regression** algorithm had better overall results.


Feature | Accuracy | Precision | Recall| F1 Score
--- | --- | --- | --- | ---
Gaussian Naive Bayes | 0.36400 | 0.19038 | 0.76800 | **0.30513**
K Nearest Neighbor | **0.81609** | 0.31148 | 0.00950 | 0.01844
**Decision Tree Classifier** | 0.76918 | 0.32305 | **0.24600** | 0.27931
**Logistical Regression** | **0.79791** | **0.36045** | 0.14400 | 0.20579

### Optimization

After reviewing the sklearn documentation, I decided to try tuning the **C**, **random_state**, **tol**, and **weight class** parameters.  Algorithm parameter tuning is an important step for improving algorithm performance and if you don't do this well it could hurt performance and hinder optimization. I decided on which parameters to tune by reviewing the sklearn documentation for each of the learning algorithms.  

In particular, class weight seems to penalize mistakes in samples of the class with the class_weight. From the documentation: "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data.  This should help with regularizing the number of POIs vs. Non-POIs.

After tuning, our two algorithms are looking pretty good!

Feature | Accuracy | Precision | Recall| F1 Score
--- | --- | --- | --- | ---
**Decision Tree Classifier** | **0.79027** | 0.39090 | 0.27500 | 0.32286
**Logistical Regression** | 0.75345 | **0.39225** | **0.64800** | **0.48869**

Let's take a look at what these numbers all mean.

## Validation - Precision,  Recall and F1 Score

To validate the efficacy of the learning algorithms, additional metrics beyond accuracy are needed because if accuracy is used as the main metric, it can lead to over fitting.  This causes a major problem since the learning algorithm would not be able to make accurate predictions about data outside the training set.  Therefore, additional validation methods are needed, specifically `Precision`, `Recall` and the `F1 Score`.

>**Recall** measures the `True Positives` divided by the `True Positive` plus `False Negative`. Out of all the items that are truly positive, how many were correctly classified as positive. Or simply, how many positive items were 'recalled' from the dataset.

>**Precision** measure `True Positive` divided by the `True Positive` plus `False Positive`. Out of all the items labeled as positive, how many truly belong to the positive class.

>** F1** is the harmonic mean of Precision and Recall.  It takes `Precision` times `Recall` and divides it by `Precision` plus `Recall`.

These metrics measure the significance of the individual predictions.

Additionally, the data is split using **Cross Validation** into a training set and test set.  This is important because it will show how well results of a will generalize to an independent data set.  In the end, the StratifiedShuffleSplit algorithm is employed to evaluate performance of the learning algorithms.  I used **Stratified Shuffle Split** in this problem over other splitting techniques available because there is a large imbalance in the distribution of the target classes.  It ensures that relative class frequencies is approximately preserved in each train and validation fold.

The final results of the **Logistic Regression** algorithm is as follows:

Feature | Accuracy | Precision | Recall| F1 Score
--- | --- | --- | --- | ---
**Logistical Regression** | **0.75345** | **0.39225** | **0.64800** | **0.48869**

The logistical regression make the following predictions:

Total Predictions | True Positives | False Positives | False Negatives | True Negatives
--- | --- | --- | ---
11000 | 1296 | 2008 | 704 | 6992

## Conclusions

From data exploration, to processing, to selecting features, to choosing a learning algorithm and finally optimization, this experiment was an excellent learning experience. It shows the power of learning from data when a pattern exists and the data is available. With these two points, machine learning is possible.

In the end, the **Logistic Regression** algorithm led to the highest Precision, Recall and F1 score among the different learning algorithms in predicting the POIs and Non-POI in the Enron Corpus.

## References

* Intro to Machine Learning - Udacity
* Machine Learning - Coursera
* Learning from Data - Edx
* CS231n: Convolutional Neural Networks for Visual Recognition - Stanford
* Neural Networks for Machine Learning - Coursera
* Bay Area Deep Learning School
* https://en.wikipedia.org/wiki/Enron
* https://en.wikipedia.org/wiki/Enron_Corpus
* https://wiki.python.org/moin/HowTo/Sorting
* http://stackoverflow.com/questions/16310015/what-does-this-mean-key-lambda-x-x1
* https://discussions.udacity.com/t/error-key-total-compensation-not-present/40365/4
* http://scikit-learn.org/stable/modules/linear_model.html
* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
* https://en.wikipedia.org/wiki/F1_score
* http://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work
