{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling the Pittsburgh OpenStreetMap Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicholas Cica\n",
    "\n",
    "nicholasjcica@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean the OpenStreetMap data for a part of Pittsburgh, PA.  Then, I will use SQL as the data schema to complete my project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will:\n",
    "\n",
    "* Assess the quality of the data for validity, accuracy, completeness, consistency and uniformity\n",
    "\n",
    "* Parse and gather data from popular file formats such as .osm and .csv\n",
    "\n",
    "* Process data from many files and very large files that can be cleaned with spreadsheet programs\n",
    "\n",
    "* Store, query, and aggregate data using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pittsburgh, PA\n",
    "* https://www.openstreetmap.org/relation/188553\n",
    "* https://mapzen.com/data/metro-extracts/metro/pittsburgh_pennsylvania/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am originally from Pittsburgh, so I thought it would be fun to revisit my old haunts and see what has changed in the past seven years since I moved away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarize Yourself with the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data and examine with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "less pittsburgh_pennsylvania.osm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the file size with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ls -l pittsburgh_pennsylvania.osm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the open street map documentation\n",
    "\n",
    "From the wiki, the beginners guide, developers section, map features, osm and eml documentation seem the most useful.  I took some time reading over the documentation before moving on.  \n",
    "https://wiki.openstreetmap.org/wiki/OSM_XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the OpenStreetMap data, some definitions were necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A **node** is one of the core elements in the OpenStreetMap data model. It consists of a single point in space defined by its latitude, longitude and node id.\n",
    "https://wiki.openstreetmap.org/wiki/Node\n",
    "\n",
    ">A **way** is an ordered list of nodes which normally also has at least one tag or is included within a Relation. Its also a path through a city - a **way** to get from one place to another.\n",
    "https://wiki.openstreetmap.org/wiki/Way\n",
    "\n",
    ">A **relation** is one of the core data elements that consists of one or more tags and also an **ordered list** of one or more nodes, ways and/or relations as **members** which is used to define logical or geographic relationships between other elements.\n",
    "https://wiki.openstreetmap.org/wiki/Relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the libararies we will use\n",
    "import re\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "OSMFILE = \"sample6.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "phone_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "state_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Counting the tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tags.keys():\n",
    "            tags[elem.tag] = 1\n",
    "        else :\n",
    "            tags[elem.tag] += 1\n",
    "    print \"Counting the Tags:\"\n",
    "    #pprint.pprint(tags)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the Tags:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'member': 422,\n",
       " 'nd': 64150,\n",
       " 'node': 55216,\n",
       " 'osm': 1,\n",
       " 'relation': 76,\n",
       " 'tag': 36855,\n",
       " 'way': 5672}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That's a lot of tags!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The osm file may be too big to load into memory, so we need to parse the data into smaller bits.  The code will find the way tags and it will return all the sub tags nested within the element for the named parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I process the data and add it into the database, I will check the \"k\" value for each \"tag\" and see if there are any potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"lower\", for tags that contain only lowercase letters and are valid,\n",
    "#\"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "#\"problemchars\", for tags with problematic characters, and\n",
    "#\"other\", for other tags that do not fall into the other three categories.\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k_val = element.get(\"k\")\n",
    "        if bool(lower.search(k_val)):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif bool(lower_colon.search(k_val)): \n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif bool(problemchars.search(k_val)):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "    #print keys\n",
    "    return keys\n",
    "\n",
    "def process_keys(filename):\n",
    "\tprint \"Now let's look for potenatial problems...\"\n",
    "\tkeys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "\tfor _, element in ET.iterparse(filename):\n",
    "\t\tkeys = key_type(element, keys)\n",
    "\t#pprint.pprint(keys)\n",
    "\treturn keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's look for potenatial problems...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lower': 16617, 'lower_colon': 18994, 'other': 1244, 'problemchars': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_keys(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tags that contain only lowercase letters and are valid: 582,904\n",
    "* Valid tags with a colon in their names: 664,822\n",
    "* Tags with problematic characters: 1\n",
    "* Tags that do not fall into the other three categories: 42,468"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out how many unique users have contributed to the map of Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_users(filename):\n",
    "\tprint \"Let's find out how many unique users have contributed to the map of Pittsburgh...\"\n",
    "\tusers = set()\n",
    "\tfor _, element in ET.iterparse(filename):\n",
    "\t\ttag = element.tag\n",
    "\t\tif tag in [ 'node', 'way', 'relation']:\n",
    "\t\t\tid = element.attrib['uid']\n",
    "\t\t\tusers.add(id)\n",
    "\tprint 'Number of Unique Users: ', len(users)\n",
    "\t#return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's find out how many unique users have contributed to the map of Pittsburgh...\n",
      "Number of Unique Users:  689\n"
     ]
    }
   ],
   "source": [
    "unique_users(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing Street Names, Phone Numbers and State Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Streets Names\n",
    "street_expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Highway\", \"Way\"]\n",
    "\n",
    "street_mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\", \n",
    "            \"Av\": \"Avenue\",\n",
    "            \"Av.\": \"Avenue\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Sq\": \"Square\",\n",
    "            }\n",
    "\n",
    "# Phone Numbers\n",
    "phone_expected = ('412-', '1-', '724-')\n",
    "\n",
    "# State Abbr,\n",
    "state_expected = [\"PA\"]\n",
    "\n",
    "state_mapping = {\"pa\": \"PA\",\n",
    "\t\t\t\t \"P\": \"PA\",\n",
    "\t\t\t\t \"Pa\": \"PA\",\n",
    "\t\t\t\t \"Ohio\": \"OH\"\n",
    "\t\t\t\t }\n",
    "\n",
    "# audit\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    # print 'street', m\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in street_expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def audit_phone_number(phone_types, phone_numbers):\n",
    "\tm = phone_type_re.search(phone_numbers)\n",
    "\t# print 'phone', m\n",
    "\tif m:\n",
    "\t\tphone_type = m.group()\n",
    "\t\t#if phone_type not in phone_expected:\n",
    "\t\tif not phone_type.startswith((phone_expected)):\n",
    "\t\t\tphone_types[phone_type].add(phone_numbers)\n",
    "\n",
    "def audit_state_type(state_types, state_name):\n",
    "    m = state_type_re.search(state_name)\n",
    "    # print 'state', m\n",
    "    if m:\n",
    "        state_type = m.group()\n",
    "        if state_type not in state_expected:\n",
    "            state_types[state_type].add(state_name)\n",
    "\n",
    "\n",
    "# Identify elements\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def is_phone_number(elem):\n",
    "    return (elem.attrib['k'] == \"phone\")\n",
    "\n",
    "# add one for states\n",
    "def is_state(elem):\n",
    "    return (elem.attrib['k'] == \"addr:state\")\n",
    "\n",
    "# Main Audit Function\n",
    "def audit(osmfile):\n",
    "\tprint \"Now its time to audit the data...\"\n",
    "\tprint \"Let's take a look at street names, phone numbers, and state abbrivations...\"\n",
    "\tprint \"\\n\"\n",
    "\tprint \"Hmm, something's fishy with our data...some human must have handled the data entry...\"\n",
    "\tprint \"\\n\"\n",
    "\tosm_file = open(osmfile, \"r\")\n",
    "\tstreet_types = defaultdict(set)\n",
    "\tphone_types = defaultdict(set)\n",
    "\tstate_types = defaultdict(set)\n",
    "\tfor event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\t\tif elem.tag == \"node\" or elem.tag == \"way\":\n",
    "\t\t\tfor tag in elem.iter(\"tag\"):\n",
    "\t\t\t\tif is_street_name(tag):\n",
    "\t\t\t\t\taudit_street_type(street_types, tag.attrib['v'])\n",
    "\t\t\t\t\t# testing update - move to shapping\n",
    "\t\t\t\t\t#better_name = update_street_name(tag.attrib['v'], street_mapping)\n",
    "\t\t\t\t\t#print 'Updated Street Name: ', better_name\n",
    "\t\t\t\tif is_phone_number(tag):\n",
    "\t\t\t\t\t# print 'Unformated Phone: ', tag.attrib['v']\n",
    "\t\t\t\t\taudit_phone_number(phone_types, tag.attrib['v'])\n",
    "\t\t\t\t\t# testing update - move to shapping\n",
    "\t\t\t\t\t#formatted_phone_number =  phone_format(tag.attrib['v'])\n",
    "\t\t\t\t\t#print 'Formated Phone: ', formatted_phone_number\n",
    "\t\t\t\tif is_state(tag):\n",
    "\t\t\t\t\t#print 'Unformated State: ', tag.attrib['v']\n",
    "\t\t\t\t\taudit_state_type(state_types, tag.attrib['v'])\n",
    "\t\t\t\t\t#better_state_name = update_state_name(tag.attrib['v'], state_mapping)\n",
    "\t\t\t\t\t#print 'Updated State Name: ', better_state_name\n",
    "\n",
    "\tosm_file.close()\n",
    "\t#print 'Street Outliers:', street_types\n",
    "\t#print \"\\n\"\n",
    "\t#print 'Phone Outliers:', phone_types\n",
    "\t#print \"\\n\"\n",
    "\t#print 'State Abbr. Outliers:', state_types\n",
    "\t#print \"\\n\"\n",
    "\treturn street_types, phone_types, state_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the **audit** of street names that don't fit the expected (\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"Way\", \"Broadway\", \"Colfax\")\n",
    "\n",
    "Perform the **audit** of Phone Numbers that are not in the correct \"555-555-5555\" formate.\n",
    "\n",
    "Perform the **audit** of States which are not abbriviated correctly to PA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now its time to audit the data...\n",
      "Let's take a look at street names, phone numbers, and state abbrivations...\n",
      "\n",
      "\n",
      "Hmm, something's fishy with our data...some human must have handled the data entry...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(defaultdict(set,\n",
       "             {'30': {'State Route 30'},\n",
       "              '51': {'Route 51'},\n",
       "              '519': {'PA 519'},\n",
       "              '885': {'Route 885'},\n",
       "              'Alley': {'Pine Alley'},\n",
       "              'Allies': {'Boulevard of the Allies'},\n",
       "              'Ave': {'Centre Ave', 'Fifth Ave', 'S Negley Ave'},\n",
       "              'Blvd': {'Washington Blvd'},\n",
       "              'Circle': {'Bardona Circle',\n",
       "               'Chardonnay Circle',\n",
       "               'Golden Circle',\n",
       "               'Jefferson Pointe Circle',\n",
       "               'Laurel Ridge Circle',\n",
       "               'Longview Circle',\n",
       "               'Marion Circle',\n",
       "               'Morning Wind Circle',\n",
       "               'Naughton Circle',\n",
       "               'Oakhurst Circle',\n",
       "               'Redrome Circle',\n",
       "               'Steeplechase Circle',\n",
       "               'Trotwood Circle',\n",
       "               'Wheatland Circle',\n",
       "               'Winners Circle'},\n",
       "              'Dr': {'Douglas Dr', 'Glengary Dr', 'Kirkwall Dr', 'Selvin Dr'},\n",
       "              'East': {'Deer Park Drive East'},\n",
       "              'Extension': {'Federal Street Extension',\n",
       "               'Middle Road Extension',\n",
       "               'Mount Troy Road Extension'},\n",
       "              'Heights': {'Meadow Heights'},\n",
       "              'Maples': {'The Maples'},\n",
       "              'Marshall': {'Marshall'},\n",
       "              'North': {'Freedom Drive North'},\n",
       "              'Pike': {'Greensburg Pike',\n",
       "               'Kittanning Pike',\n",
       "               'Washington Pike'},\n",
       "              'Rd': {'Ravencrest Rd', 'Robinhood Rd'},\n",
       "              'Rear': {'Spruce Street Rear'},\n",
       "              'ST': {'12TH ST'},\n",
       "              'South': {'Randolph Drive South'},\n",
       "              'Ter': {'Faber Ter'},\n",
       "              'Terrace': {'Chartiers Terrace', 'Riverview Terrace'},\n",
       "              'West': {'Deer Park Drive West', 'Waterman Road West'}}),\n",
       " defaultdict(set,\n",
       "             {'1.800.405.0935': {'+ 1.800.405.0935'},\n",
       "              '1613': {'+1 888 865 1613'},\n",
       "              '1842': {'+1 412 678 1842'},\n",
       "              '243-9979': {'(304) 243-9979'},\n",
       "              '3500': {'1 724 873 3500'},\n",
       "              '4040': {'+1 412 761 4040'},\n",
       "              '412)431-1114': {'(412)431-1114'},\n",
       "              '412.381.6000': {'412.381.6000'},\n",
       "              '412.431.1810': {'412.431.1810'},\n",
       "              '431-1450': {'(412) 431-1450'},\n",
       "              '4405': {'+1 412 741 4405'},\n",
       "              '4800': {'+1 412 821 4800'},\n",
       "              '5620': {'+1 412 793 5620'},\n",
       "              '621-4951': {'(412) 621-4951'},\n",
       "              '7402642042': {'7402642042'},\n",
       "              '8810': {'+1 412 367 8810'}}),\n",
       " defaultdict(set, {'OH': {'OH'}, 'WV': {'WV'}}))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Database - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so I will clean and parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print \"Now we have to clean up their mess!\"\n",
    "def update_street_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    better_name = name\n",
    "    if m:\n",
    "        if m.group() in mapping.keys():\n",
    "            #print 'BEFORE'\n",
    "            #print name\n",
    "            better_name = re.sub(m.group(),mapping[m.group()], name)\n",
    "            #print 'AFTER'\n",
    "            #print better_name\n",
    "        # else:\n",
    "            # print 'Not in Mapping:', m.group()\n",
    "    return better_name\n",
    "\n",
    "def phone_format(phone_number):\n",
    "    clean_phone_number = re.sub('[^0-9]+', '', phone_number)\n",
    "    formatted_phone_number = re.sub(\"(\\d)(?=(\\d{3})+(?!\\d))\", r\"\\1-\", \"%d\" % int(clean_phone_number[:-1])) + clean_phone_number[-1]\n",
    "    return formatted_phone_number\n",
    "\n",
    "def update_state_name(name, mapping):\n",
    "\tm = state_type_re.search(name)\n",
    "\tbetter_name = name\n",
    "\tif m:\n",
    "\t\tif m.group() in mapping.keys():\n",
    "\t\t\t#print 'BEFORE'\n",
    "\t\t\t#print name\n",
    "\t\t\tbetter_name = re.sub(m.group(),mapping[m.group()], name)\n",
    "\t\t\t#print 'AFTER'\n",
    "\t\t\t#print better_name\n",
    "\t\t# else:\n",
    "\t\t\t# print 'Not in Mapping:', m.group()\n",
    "\treturn better_name\n",
    "\n",
    "\n",
    "# SHAPE\n",
    "OSM_PATH = \"sample6.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "\t\t\t\t\tproblem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "\t\"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "\tnode_attribs = {}\n",
    "\tway_attribs = {}\n",
    "\tway_nodes = []\n",
    "\ttags = [] \n",
    "\n",
    "\tif element.tag == 'node':\n",
    "\t\tfor node in NODE_FIELDS:\n",
    "\t\t\tnode_attribs[node] = element.attrib[node]\n",
    "\t\tfor child in element:\n",
    "\t\t\ttag = {}\n",
    "\t\t\tif PROBLEMCHARS.search(child.attrib[\"k\"]):\n",
    "\t\t\t\tcontinue\n",
    "\t\n",
    "\t\t\telif LOWER_COLON.search(child.attrib[\"k\"]):\n",
    "\t\t\t\n",
    "\t\t\t\ttag_type = child.attrib[\"k\"].split(':',1)[0]\n",
    "\t\t\t\ttag_key = child.attrib[\"k\"].split(':',1)[1]\n",
    "\t\t\t\ttag[\"key\"] = tag_key\n",
    "\t\t\t\tif tag_type:\n",
    "\t\t\t\t\ttag[\"type\"] = tag_type\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag[\"type\"] = 'unspecified'\n",
    "\t\t\t\n",
    "\t\t\t\ttag[\"id\"] = element.attrib[\"id\"]\n",
    "\t\t\t\tif child.attrib['k'] == \"addr:street\":\n",
    "\t\t\t\t\ttag['value'] = update_street_name(child.attrib['v'], street_mapping)\n",
    "\t\t\t\t\t#print \"Node (problem) address updated\"\n",
    "\t\t\t\tif child.attrib['k'] == \"phone\":\n",
    "\t\t\t\t\ttag['value'] = phone_format(child.attrib['v'])\n",
    "\t\t\t\t\t#print \"Node (problem) phone updated\"\n",
    "\t\t\t\tif child.attrib['k'] == \"addr:state\":\n",
    "\t\t\t\t\ttag['value'] = update_state_name(child.attrib['v'], state_mapping)\n",
    "\t\t\t\t\t#print \"Node (problem) state updated\"\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag[\"value\"] = child.attrib[\"v\"]\n",
    "\t\t\telse:\n",
    "\t\t\t\tif child.attrib['k'] == \"addr:street\":\n",
    "\t\t\t\t\ttag['value'] = update_street_name(child.attrib['v'], street_mapping)\n",
    "\t\t\t\t\t#print \"Node address updated\"\n",
    "\t\t\t\tif child.attrib['k'] == \"phone\":\n",
    "\t\t\t\t\ttag['value'] = phone_format(child.attrib['v'])\n",
    "\t\t\t\t\t#print \"Node phone updated\"\n",
    "\t\t\t\tif child.attrib['k'] == \"addr:state\":\n",
    "\t\t\t\t\ttag['value'] = update_state_name(child.attrib['v'], state_mapping)\n",
    "\t\t\t\t\t#print \"Node state updated\"\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag[\"value\"] = child.attrib[\"v\"]\n",
    "\t\t\t\ttag[\"key\"] = child.attrib[\"k\"]\n",
    "\t\t\t\ttag[\"type\"] = \"unspecified\"\n",
    "\t\t\t\ttag[\"id\"] = element.attrib[\"id\"]\n",
    "\t\t\tif tag:\n",
    "\t\t\t\ttags.append(tag)\n",
    "\t\t\treturn {'node': node_attribs, 'node_tags': tags}\n",
    "\t\n",
    "\telif element.tag == 'way':\n",
    "\t\tfor way in WAY_FIELDS:\n",
    "\t\t\tway_attribs[way] = element.attrib[way]\n",
    "\t\tfor child in element:\n",
    "\t\t\tnd = {}\n",
    "\t\t\ttag = {}\n",
    "\t\t\tif child.tag == 'tag':\n",
    "\t\t\t\tif PROBLEMCHARS.search(child.attrib[\"k\"]):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telif LOWER_COLON.search(child.attrib[\"k\"]):\n",
    "\t\t\t\t\ttag_type = child.attrib[\"k\"].split(':',1)[0]\n",
    "\t\t\t\t\ttag_key = child.attrib[\"k\"].split(':',1)[1]\n",
    "\t\t\t\t\ttag[\"key\"] = tag_key\n",
    "\t\t\t\t\tif tag_type:\n",
    "\t\t\t\t\t\ttag[\"type\"] = tag_type\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttag[\"type\"] = 'unspecified'\n",
    "\t\t\t\t\ttag[\"id\"] = element.attrib[\"id\"]\n",
    "\t\t\t\t\tif child.attrib['k'] == \"addr:street\":\n",
    "\t\t\t\t\t\ttag['value'] = update_street_name(child.attrib['v'], street_mapping)\n",
    "\t\t\t\t\t\t#print \"Way (problem) address updated\"\n",
    "\t\t\t\t\tif child.attrib['k'] == \"phone\":\n",
    "\t\t\t\t\t\ttag['value'] = phone_format(child.attrib['v'])\n",
    "\t\t\t\t\t\t#print \"Way (problem) phone updated\"\n",
    "\t\t\t\t\tif child.attrib['k'] == \"addr:state\":\n",
    "\t\t\t\t\t\ttag['value'] = update_state_name(child.attrib['v'], state_mapping)\n",
    "\t\t\t\t\t\t#print \"Way (problem) state updated\"\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttag[\"value\"] = child.attrib[\"v\"]\n",
    "\t\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tif child.attrib['k'] == \"addr:street\":\n",
    "\t\t\t\t\t\ttag['value'] = update_street_name(child.attrib['v'], street_mapping)\n",
    "\t\t\t\t\t\t#print \"Way address updated\"\n",
    "\t\t\t\t\tif child.attrib['k'] == \"phone\":\n",
    "\t\t\t\t\t\ttag['value'] = phone_format(child.attrib['v'])\n",
    "\t\t\t\t\t\t#print \"Way phone updated\"\n",
    "\t\t\t\t\tif child.attrib['k'] == \"addr:state\":\n",
    "\t\t\t\t\t\ttag['value'] = update_state_name(child.attrib['v'], state_mapping)\n",
    "\t\t\t\t\t\t#print \"Way state updated\"\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttag[\"value\"] = child.attrib[\"v\"]\n",
    "\t\t\t\t\ttag[\"key\"] = child.attrib[\"k\"]\n",
    "\t\t\t\t\ttag[\"type\"] = \"unspecified\"\n",
    "\t\t\t\t\ttag[\"id\"] = element.attrib[\"id\"]\n",
    "\t\t\t\tif tag:\n",
    "\t\t\t\t\ttags.append(tag)\n",
    "\n",
    "\t\t\telif child.tag == 'nd':\n",
    "\t\t\t\tnd['id'] = element.attrib[\"id\"]\n",
    "\t\t\t\tnd['node_id'] = child.attrib[\"ref\"]\n",
    "\t\t\t\tnd['position'] = len(way_nodes)\n",
    "\t\n",
    "\t\t\t\tif nd:\n",
    "\t\t\t\t\tway_nodes.append(nd)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\treturn {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "    print \"Now Let's clean and prepare the data for our database!\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    " \tprint \"Data processing complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok!  let's process the data Map of Pittsburgh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Let's clean and prepare the data for our database!\n",
      "Data processing complete!\n"
     ]
    }
   ],
   "source": [
    "process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process has split the osm file into five cvs files:\n",
    "\n",
    "* nodes_tags.csv\n",
    "* nodes.csv\n",
    "* ways.csv\n",
    "* ways_tags.csv\n",
    "* ways_nodes.csv\n",
    "\n",
    "Now we need to set up the database and get those files into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the cleaned .csv files into a SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sqlite3 pittsburgh.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Tables (Schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Tables were Created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".tables\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".mode csv\n",
    ".import nodes_tags.csv nodes_tags\n",
    ".import nodes.csv nodes\n",
    ".import ways.csv ways\n",
    ".import ways_tags.csv ways_tags\n",
    ".import ways_nodes.csv ways_nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet!  Now we can run all kinds of crazy queries against the database and find out cool things about Pittsburgh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains basic statistics about the dataset, the SQL queries used to gather them, the DB API python code, and some additional ideas about the data in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pittsburgh_pennsylvania.osm: 431.6 MB\n",
    "* pittsburgh.db: 237.6 MB\n",
    "* nodes.csv: 162 MB\n",
    "* nodes_tags.csv: 9.8 MB\n",
    "* ways.csv: 11.9 MB\n",
    "* ways_tags.csv: 34.2 MB\n",
    "* ways_nodes.csv: 53.1 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes:  1932541\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT COUNT(*) FROM nodes;\"\n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"Number of Nodes: \", rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ways:  198508\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT COUNT(*) FROM ways;\"\n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"Number of Ways: \", rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Users:  1253\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT COUNT(DISTINCT(u.uid)) \\\n",
    "FROM (SELECT uid \\\n",
    "      FROM nodes \\\n",
    "      UNION ALL \\\n",
    "      SELECT uid \\\n",
    "      FROM ways) u;\" \n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"Number of Unique Users: \", rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Contributing Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Contributing Users:  [(u'GeoKitten', 261073), (u'woodpeck_fixbot', 204981), (u'rickmastfan67', 128972), (u'behemoth14', 101086), (u'Gary Hayden', 95026), (u'abbafei', 93662), (u'Fredlyfish4', 80402), (u'dchiles', 72425), (u'AndrewSnow', 72365), (u'TIGERcnl', 63198)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT u.user, COUNT(*) as num \\\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) u \\\n",
    "GROUP BY u.user \\\n",
    "ORDER BY num DESC \\\n",
    "LIMIT 10;\" \n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"Top 10 Contributing Users: \", rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Users Appearing Only Once (having 1 post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users Appearing Only Once:  181\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT COUNT(*) FROM (SELECT u.user, COUNT(*) as num \\\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) u \\\n",
    "GROUP BY u.user \\\n",
    "HAVING num = 1);\" \n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"Number of Users Appearing Only Once: \", rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Top 20 Amenities in Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Top 20 Amenities in Pittsburgh:  [(u'place_of_worship', 1444), (u'school', 1209), (u'restaurant', 660), (u'grave_yard', 374), (u'fast_food', 291), (u'library', 226), (u'post_office', 200), (u'bench', 191), (u'fuel', 188), (u'waste_basket', 175), (u'parking', 171), (u'bank', 143), (u'post_box', 134), (u'police', 126), (u'cafe', 100), (u'kindergarten', 95), (u'pharmacy', 92), (u'fire_station', 85), (u'bar', 75), (u'toilets', 72)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT value, COUNT(*) as num \\\n",
    "FROM nodes_tags \\\n",
    "WHERE key = 'amenity' \\\n",
    "GROUP BY value \\\n",
    "ORDER BY num DESC \\\n",
    "LIMIT 20;\" \n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"List of Top 20 Amenities in Pittsburgh: \", rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Starbucks are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Subway', 49), (u\"McDonald's\", 43), (u'Giant Eagle', 33), (u'Rite Aid', 28), (u'PNC Bank', 26), (u'Starbucks', 25), (u\"Wendy's\", 24), (u'GetGo', 23), (u'Hydrant', 21), (u'Sheetz', 21), (u'Pizza Hut', 19), (u'Burger King', 18), (u\"Arby's\", 16), (u'Sunoco', 16), (u'Dairy Queen', 15), (u'Dollar General', 14), (u'First Baptist Church', 12), (u'H&R Block', 12), (u'Saint Johns Church', 12), (u'CVS', 11)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT value, COUNT(*) as num \\\n",
    "FROM nodes_tags \\\n",
    "WHERE key = 'name' \\\n",
    "GROUP BY value \\\n",
    "ORDER BY num DESC \\\n",
    "LIMIT 20;\" \n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Top 5 Cuisine in Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Top 5 Cuisine in Pittsburgh:  [(u'american', 131), (u'pizza', 77), (u'italian', 38), (u'chinese', 30), (u'mexican', 20)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "FROM nodes_tags \\\n",
    "JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value = 'restaurant') r ON nodes_tags.id = r.id \\\n",
    "WHERE nodes_tags.key = 'cuisine' \\\n",
    "GROUP BY nodes_tags.value \\\n",
    "ORDER BY num DESC \\\n",
    "LIMIT 5;\" \n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"List of Top 5 Cuisine in Pittsburgh: \", rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Religions at a Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Religions at a Glance:  [(u'christian', 1360), (u'jewish', 14), (u'muslim', 3), (u'unitarian_universalist', 2), (u'eckankar', 1)]\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"pittsburgh.db\")\n",
    "c = db.cursor()\n",
    "query = \"SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "FROM nodes_tags \\\n",
    "JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='place_of_worship') r ON nodes_tags.id = r.id \\\n",
    "WHERE nodes_tags.key='religion' \\\n",
    "GROUP BY nodes_tags.value \\\n",
    "ORDER BY num DESC \\\n",
    "LIMIT 5;\" \n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "db.close()\n",
    "\n",
    "print \"Religions at a Glance: \", rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprising that Christianity is well represented. Pittsburgh has some of the most beautfil churches that also brew beer.\n",
    "\n",
    "Apparently, **Eckankar** (meaning Co-worker with God) is a modern-day religion where followers believe its purpose is to help individuals find their way back to God through direct personal spiritual experiences.  Huh, learn something everyday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Improving and Analyzing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the next step would be to visualize the data and look for relationships.  If we were really bold, we could use this data from one American city and use machine learning to predict outcomes about other, similarly sized American cities and maybe even be able to make predictions based on population size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits and Anticipated Problems in Implementing the Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of predicting city data from other city data is measuring the effectives of public works.  We may be able to utilize a prediction based on data by measuring the number of police stations, fire stations, hospitals, etc. against the crime rate, emergency first response time, and level of health care. Since our data was entered by hand (in this case), we need to be careful if our predicted data doesnâ€™t match the actual data. We need to use other metrics to measure the validity of the data to be sure that the predictions are within an accepted range.\n",
    "\n",
    "The problem we could face in implimenting such an analysis is that:\n",
    "1. we need the data\n",
    "2. a pattern must exist\n",
    "\n",
    "Without those two things, we would not be able to learn from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is definietly something to be said by the amount of human error that goes into a dataset such as this.  Only a domain expert of the area would be able to - at a glance - clean the data on a case by case basis.  That's why using these data analysis tools are important.  It allows us to explore the data and look for outliers and then clean the data without having to know the ins and outs of all the data we are sifting through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shout out to the DataScience slack group for helping me figure out how to import the data into my database!\n",
    "https://udacitydatascience.slack.com/messages/p3-openstreetmap/\n",
    "\n",
    "Markdown help: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#code\n",
    "\n",
    "Wikipedia for fact checking this Eckankar religion: https://en.wikipedia.org/wiki/Eckankar"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
